{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from lavis.models import load_model_and_preprocess\n",
    "from transformers.generation import GenerationConfig\n",
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"lavis/output/pp_14b/sft/global_step296/unfreeze_llm_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing Initializing Vision-Encoder...\n",
      "Finishing Loading Q-former Initializing Config...\n",
      "Finishing Initializing Q-former...\n",
      "Loading LLM:/root/autodl-tmp/MiniGPT4Qwen/lavis/../cache/ckpt/Qwen-14B-Chat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers_modules.Qwen-14B-Chat.modeling_qwen:The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "WARNING:transformers_modules.Qwen-14B-Chat.modeling_qwen:Try importing flash-attention for faster inference...\n",
      "WARNING:transformers_modules.Qwen-14B-Chat.modeling_qwen:Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "WARNING:transformers_modules.Qwen-14B-Chat.modeling_qwen:Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "WARNING:transformers_modules.Qwen-14B-Chat.modeling_qwen:Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008426427841186523,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 15,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933b16c65ad24c7eb93db5f3b09c88c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeze LLM...\n",
      "Start loading pretrained model: /root/autodl-tmp/MiniGPT4Qwen/lavis/../cache/ckpt/blip2/blip2_pretrained_flant5xxl.pth\n",
      "Loading the File Named: /root/autodl-tmp/MiniGPT4Qwen/lavis/../cache/ckpt/blip2/blip2_pretrained_flant5xxl.pth...\n",
      "Checkpoint: lavis/output/pp_14b/sft/global_step296/unfreeze_llm_model.pth\n"
     ]
    }
   ],
   "source": [
    "load_model_and_preprocess = partial(load_model_and_preprocess,is_eval=True,device=\"cuda\")\n",
    "llm_device_map=\"auto\"\n",
    "model, vis_processors, _ = load_model_and_preprocess(\"minigpt4qwen\", \"qwen14b_chat\",llm_device_map=llm_device_map)\n",
    "model.load_checkpoint(checkpoint_path)\n",
    "# llm_state_dict = model.llm_model.state_dict()\n",
    "\n",
    "generation_config = {\n",
    "    \"chat_format\": \"chatml\",\n",
    "    \"eos_token_id\": 151643,\n",
    "    \"pad_token_id\": 151643,\n",
    "    \"max_window_size\": 6144,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"do_sample\": False,\n",
    "    \"transformers_version\": \"4.31.0\"\n",
    "}\n",
    "\n",
    "generation_config = GenerationConfig.from_dict(generation_config)\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./examples/zjm.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "query = \"<Img><ImageHere></Img> 用简短一句话描述这幅图片\"\n",
    "\n",
    "image_tensor = vis_processors['eval'](image).unsqueeze(dim=0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "[('<Img><ImageHere></Img> 用简短一句话描述这幅图片', '0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000')]\n"
     ]
    }
   ],
   "source": [
    "history, response = [], ''\n",
    "with torch.cuda.amp.autocast(enabled=True,dtype=torch.bfloat16):\n",
    "    response, history = model.chat(query, history=history, image_tensor=image_tensor, generation_config=generation_config)\n",
    "\n",
    "print(response)\n",
    "print(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
