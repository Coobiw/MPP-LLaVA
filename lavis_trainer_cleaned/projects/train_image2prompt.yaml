model:
  arch: "resnet50_clip_image2prompt"
  input_resolution: 512
  output_dim: 1024
  freeze_bn: true
  head_scale: 10.
  use_mlp_head: true
  dropout_rate: 0.

  # 一般测试，或者需要进一步load预训练模型的时候才采用
  load_finetuned: false
  finetuned: ""

dataset:
  train_cfg:
    name: 'image2prompt_dataset'
    transform:
      name: "image_train_processor"
      cfg:
        image_size: 512
    data_path: example_data/image2prompt/anno.json
  val_cfg:
    name: 'image2prompt_dataset'
    transform:
      name: "image_eval_processor"
      cfg:
        image_size: 512
    data_path: example_data/image2prompt/anno.json # image2prompt没有实现eval，所以随便写一个就好，不会实际进行

run:
  task: "image2prompt"
  resume_ckpt_path: null
  # scheduler
  lr_sched: "linear_warmup_cosine_lr"
  lr_decay_rate: null # cosine的情况不需要，step_lr才需要
  warmup_lr: 0 # 小于0代表则直接赋值为init_lr了，这样就跳过warmup，第一个epoch直接constant了
  warmup_steps: 20 # warmup 的iterations数目
  min_lr: 1e-6 # 衰减到的最小的lr
  # optimizer
  init_lr: 1e-5 # warmup结束的lr，不启动warmup就是最开始的lr
  lr_layer_decay: 1
  weight_decay: 0.
  beta2: 0.999 # AdamW优化器的beta2参数

  batch_size: 4 # 每个gpu上的bs
  batch_size_val: 4
  num_worker: 4 # 每个gpu上的workers数目
  max_epoch: 10
  log_freq: 1 # 多少个iteration进行print一次log
  accum_grad_iters: 1 # 梯度累积
  grad_norm_clip: null # 梯度裁剪

  output_dir: "output/examples/image2prompt"

  evaluate: False # 如果开启，则只进行evaluate，不进行训练
  eval_freq: 1 # 多少个epoch对模型进行一次evaluate
  save_freq: 1 # 多少个epoch保存一次模型

  amp: False # 混合精度
  device: "cuda"
  distributed: True
  dist_url: "env://" # 代表ddp的参数采用环境变量里的值